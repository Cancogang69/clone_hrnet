{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":8496202,"datasetId":5069685,"databundleVersionId":8637246},{"sourceType":"datasetVersion","sourceId":8781138,"datasetId":5071718,"databundleVersionId":8937432},{"sourceType":"datasetVersion","sourceId":8498932,"datasetId":5071683,"databundleVersionId":8640137}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install yacs ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:32:54.237548Z","iopub.execute_input":"2024-06-25T09:32:54.237925Z","iopub.status.idle":"2024-06-25T09:33:06.286371Z","shell.execute_reply.started":"2024-06-25T09:32:54.237869Z","shell.execute_reply":"2024-06-25T09:33:06.285262Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: yacs in /opt/conda/lib/python3.10/site-packages (0.1.8)\nRequirement already satisfied: ultralytics in /opt/conda/lib/python3.10/site-packages (8.2.42)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs) (6.0.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport os\nimport logging\nfrom yacs.config import CfgNode as CN\nfrom easydict import EasyDict as edict\nfrom collections import OrderedDict\nimport math\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:06.288534Z","iopub.execute_input":"2024-06-25T09:33:06.288913Z","iopub.status.idle":"2024-06-25T09:33:12.517448Z","shell.execute_reply.started":"2024-06-25T09:33:06.288858Z","shell.execute_reply":"2024-06-25T09:33:12.516670Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"CTX = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEBUG = True\ncudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.enabled = True\n\nMPII_KEYPOINT_INDEXES = {\n  0: \"right ankle\",\n  1: \"right knee\",\n  2: \"right hip\", \n  3: \"left hip\", \n  4: \"left knee\", \n  5: \"left ankle\",\n  6: \"pelvis\", \n  7: \"thorax\", \n  8: \"upper neck\", \n  9: \"head top\", \n  10: \"right wrist\",\n  11: \"right elbow\", \n  12: \"right shoulder\", \n  13: \"left shoulder\", \n  14: \"left elbow\",\n  15: \"left wrist\"\n}\nNUM_KPTS = len(MPII_KEYPOINT_INDEXES)\nSKELETON = {\n  \"left_lower_leg\": [0, 1], \n  \"left_thigh\": [2, 1], \n  \"left_hip\": [2, 6], \n  \"right_lower_leg\": [5, 4],\n  \"right_thigh\": [3, 4],\n  \"right_hip\": [3, 6],\n  \"torso\": [6, 7], \n  \"neck\": [7, 8],             #actually it's thorax - upper neck \n  \"head\": [8, 9],\n  \"right_forearm\": [10, 11],\n  \"right_upper_arm\": [11, 12], \n  \"right_shoulder\": [12, 7],\n  \"left_forearm\": [15, 14],\n  \"left_upper_arm\": [14, 13], \n  \"left_shoulder\": [13, 7]\n}\nSQUAT_PART = [\"left_lower_leg\", \"left_thigh\", \"right_lower_leg\",\n              \"right_thigh\", \"torso\"]\nSQUAT_KEYPART = [\n  [\"left_lower_leg\", \"left_thigh\"],\n  [\"right_lower_leg\", \"right_thigh\"]\n]\nSQUAT_STAGE_ANGLE = [[180, 170], [170, 155],\n               [155, 137], [137, 114], [114, 0]]\n\nJUMPING_JACK_PART = [\"left_lower_leg\", \"left_thigh\", \n  \"right_lower_leg\", \"right_thigh\",\n  \"torso\", \"right_forearm\", \"right_upper_arm\",\n  \"left_forearm\", \"left_upper_arm\"]\n\nJUMPING_JACK_KEYPART = [\n  [\"left_upper_arm\", \"torso\"],\n  [\"right_upper_arm\", \"torso\"]\n]\n\nJUMPING_JACK_STAGE_ANGLE = [[0, 10], [10, 35], [35, 64],\n                       [64, 104], [104, 180]]\n\nCOLOR = {\n  \"red\": [0, 0, 255],\n  \"blue\": [255, 0, 0],\n  \"green\": [0, 255, 0]\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:49:58.780416Z","iopub.execute_input":"2024-06-25T09:49:58.781406Z","iopub.status.idle":"2024-06-25T09:49:58.797532Z","shell.execute_reply.started":"2024-06-25T09:49:58.781364Z","shell.execute_reply":"2024-06-25T09:49:58.796637Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Resnet","metadata":{}},{"cell_type":"code","source":"resnet_config = edict()\n\nresnet_config.OUTPUT_DIR = ''\nresnet_config.LOG_DIR = ''\nresnet_config.DATA_DIR = ''\nresnet_config.GPUS = '0'\nresnet_config.WORKERS = 4\nresnet_config.PRINT_FREQ = 20\n\n\n# pose_resnet related params\nPOSE_RESNET = edict()\nPOSE_RESNET.NUM_LAYERS = 50\nPOSE_RESNET.DECONV_WITH_BIAS = False\nPOSE_RESNET.NUM_DECONV_LAYERS = 3\nPOSE_RESNET.NUM_DECONV_FILTERS = [256, 256, 256]\nPOSE_RESNET.NUM_DECONV_KERNELS = [4, 4, 4]\nPOSE_RESNET.FINAL_CONV_KERNEL = 1\nPOSE_RESNET.TARGET_TYPE = 'gaussian'\nPOSE_RESNET.HEATMAP_SIZE = [64, 64]  # width * height, ex: 24 * 32\nPOSE_RESNET.SIGMA = 2\n\nMODEL_EXTRAS = {\n    'pose_resnet': POSE_RESNET,\n}\n\n# common params for NETWORK\nresnet_config.MODEL = edict()\nresnet_config.MODEL.NAME = 'pose_resnet'\nresnet_config.MODEL.INIT_WEIGHTS = True\nresnet_config.MODEL.PRETRAINED = '/kaggle/input/cva-models/checkpoint.pth.tar'\nresnet_config.MODEL.NUM_JOINTS = 16\nresnet_config.MODEL.IMAGE_SIZE = [256, 256]  # width * height, ex: 192 * 256\nresnet_config.MODEL.EXTRA = MODEL_EXTRAS[resnet_config.MODEL.NAME]\n\nresnet_config.MODEL.STYLE = 'pytorch'\n\nresnet_config.LOSS = edict()\nresnet_config.LOSS.USE_TARGET_WEIGHT = True\n\n# DATASET related params\nresnet_config.DATASET = edict()\nresnet_config.DATASET.FLIP = True\nresnet_config.DATASET.SCALE_FACTOR = 0.25\nresnet_config.DATASET.ROT_FACTOR = 30\n\nposeres_cfg = edict(resnet_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:12.561552Z","iopub.execute_input":"2024-06-25T09:33:12.562277Z","iopub.status.idle":"2024-06-25T09:33:12.574655Z","shell.execute_reply.started":"2024-06-25T09:33:12.562250Z","shell.execute_reply":"2024-06-25T09:33:12.573840Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## HRnet","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"HRnet_cfg = CN()\n\nHRnet_cfg.OUTPUT_DIR = 'output'\nHRnet_cfg.LOG_DIR = 'log'\nHRnet_cfg.DATA_DIR = ''\nHRnet_cfg.GPUS = (0,)\nHRnet_cfg.WORKERS = 24\nHRnet_cfg.PRINT_FREQ = 100\nHRnet_cfg.AUTO_RESUME = True\nHRnet_cfg.PIN_MEMORY = True\nHRnet_cfg.RANK = 0\n\n# common params for NETWORK\nHRnet_cfg.MODEL = CN()\nHRnet_cfg.MODEL.NAME = 'pose_hrnet'\nHRnet_cfg.MODEL.INIT_WEIGHTS = True\nHRnet_cfg.MODEL.PRETRAINED = '/kaggle/input/cva-models/hrpose_w32_256x256.pth'\nHRnet_cfg.MODEL.NUM_JOINTS = 16\nHRnet_cfg.MODEL.TAG_PER_JOINT = True\nHRnet_cfg.MODEL.TARGET_TYPE = 'gaussian'\nHRnet_cfg.MODEL.IMAGE_SIZE = [256, 256]  # width * height, ex: 192 * 256\nHRnet_cfg.MODEL.HEATMAP_SIZE = [64, 64]  # width * height, ex: 24 * 32\nHRnet_cfg.MODEL.SIGMA = 2\n\nHRnet_cfg.MODEL.EXTRA = CN()\nHRnet_cfg.MODEL.EXTRA.PRETRAINED_LAYERS = [\n  'conv1',\n  'bn1',\n  'conv2',\n  'bn2',\n  'layer1',\n  'transition1',\n  'stage2',\n  'transition2',\n  'stage3',\n  'transition3',\n  'stage4'\n]\nHRnet_cfg.MODEL.EXTRA.STEM_INPLANES = 64\nHRnet_cfg.MODEL.EXTRA.FINAL_CONV_KERNEL = 1\n\nHRnet_cfg.MODEL.EXTRA.STAGE2 = CN()\nHRnet_cfg.MODEL.EXTRA.STAGE2.NUM_MODULES = 1\nHRnet_cfg.MODEL.EXTRA.STAGE2.NUM_BRANCHES = 2\nHRnet_cfg.MODEL.EXTRA.STAGE2.NUM_BLOCKS = [4, 4]\nHRnet_cfg.MODEL.EXTRA.STAGE2.NUM_CHANNELS = [32, 64]\nHRnet_cfg.MODEL.EXTRA.STAGE2.BLOCK = 'BASIC'\nHRnet_cfg.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n\nHRnet_cfg.MODEL.EXTRA.STAGE3 = CN()\nHRnet_cfg.MODEL.EXTRA.STAGE3.NUM_MODULES = 4\nHRnet_cfg.MODEL.EXTRA.STAGE3.NUM_BRANCHES = 3\nHRnet_cfg.MODEL.EXTRA.STAGE3.NUM_BLOCKS = [4, 4, 4]\nHRnet_cfg.MODEL.EXTRA.STAGE3.NUM_CHANNELS = [32, 64, 128]\nHRnet_cfg.MODEL.EXTRA.STAGE3.BLOCK = 'BASIC'\nHRnet_cfg.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n\nHRnet_cfg.MODEL.EXTRA.STAGE4 = CN()\nHRnet_cfg.MODEL.EXTRA.STAGE4.NUM_MODULES = 3\nHRnet_cfg.MODEL.EXTRA.STAGE4.NUM_BRANCHES = 4\nHRnet_cfg.MODEL.EXTRA.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\nHRnet_cfg.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [32, 64, 128, 256]\nHRnet_cfg.MODEL.EXTRA.STAGE4.BLOCK = 'BASIC'\nHRnet_cfg.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n\nHRnet_cfg.LOSS = CN()\nHRnet_cfg.LOSS.USE_TARGET_WEIGHT = True\n\n# DATASET related params\nHRnet_cfg.DATASET = CN()\nHRnet_cfg.DATASET.ROOT = '/kaggle/input/mpii-2014'\nHRnet_cfg.DATASET.DATASET = 'mpii'\nHRnet_cfg.DATASET.TRAIN_SET = 'train'\nHRnet_cfg.DATASET.TEST_SET = 'valid'\nHRnet_cfg.DATASET.DATA_FORMAT = 'jpg'\nHRnet_cfg.DATASET.HYBRID_JOINTS_TYPE = ''\nHRnet_cfg.DATASET.SELECT_DATA = False\n\n# training data augmentation\nHRnet_cfg.DATASET.COLOR_RGB = True\nHRnet_cfg.DATASET.FLIP = True\nHRnet_cfg.DATASET.NUM_JOINTS_HALF_BODY = 8\nHRnet_cfg.DATASET.PROB_HALF_BODY = -1.0\nHRnet_cfg.DATASET.ROT_FACTOR = 30\nHRnet_cfg.DATASET.SCALE_FACTOR = 0.25\n\n# train\nHRnet_cfg.TRAIN = CN()\n\nHRnet_cfg.TRAIN.LR_FACTOR = 0.1\nHRnet_cfg.TRAIN.LR_STEP = [170, 200]\nHRnet_cfg.TRAIN.LR = 0.001\n\nHRnet_cfg.TRAIN.OPTIMIZER = 'adam'\nHRnet_cfg.TRAIN.MOMENTUM = 0.9\nHRnet_cfg.TRAIN.WD = 0.0001\nHRnet_cfg.TRAIN.NESTEROV = False\nHRnet_cfg.TRAIN.GAMMA1 = 0.99\nHRnet_cfg.TRAIN.GAMMA2 = 0.0\n\nHRnet_cfg.TRAIN.BEGIN_EPOCH = 0\nHRnet_cfg.TRAIN.END_EPOCH = 100\n\nHRnet_cfg.TRAIN.RESUME = False\nHRnet_cfg.TRAIN.CHECKPOINT = ''\n\nHRnet_cfg.TRAIN.BATCH_SIZE_PER_GPU = 32\nHRnet_cfg.TRAIN.SHUFFLE = True\n\n# testing\nHRnet_cfg.TEST = CN()\nHRnet_cfg.TEST.MODEL_FILE = \"/kaggle/input/cva-models/hrpose_w32_256x256.pth\"\nHRnet_cfg.TEST.BATCH_SIZE_PER_GPU = 32\nHRnet_cfg.TEST.FLIP_TEST = True\nHRnet_cfg.TEST.POST_PROCESS = True\nHRnet_cfg.TEST.SHIFT_HEATMAP = True\nHRnet_cfg.TEST.USE_GT_BBOX = False\n\n\n# debug\nHRnet_cfg.DEBUG = CN()\nHRnet_cfg.DEBUG.DEBUG = False\nHRnet_cfg.DEBUG.SAVE_BATCH_IMAGES_GT = False\nHRnet_cfg.DEBUG.SAVE_BATCH_IMAGES_PRED = False\nHRnet_cfg.DEBUG.SAVE_HEATMAPS_GT = False\nHRnet_cfg.DEBUG.SAVE_HEATMAPS_PRED = False","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:12.576219Z","iopub.execute_input":"2024-06-25T09:33:12.576543Z","iopub.status.idle":"2024-06-25T09:33:12.601528Z","shell.execute_reply.started":"2024-06-25T09:33:12.576512Z","shell.execute_reply":"2024-06-25T09:33:12.600690Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## DarkPose","metadata":{}},{"cell_type":"code","source":"DarkPose_cfg = CN()\n\nDarkPose_cfg.OUTPUT_DIR = 'output'\nDarkPose_cfg.LOG_DIR = 'log'\nDarkPose_cfg.DATA_DIR = ''\nDarkPose_cfg.GPUS = (0,)  # Updated\nDarkPose_cfg.WORKERS = 24\nDarkPose_cfg.PRINT_FREQ = 100\nDarkPose_cfg.AUTO_RESUME = True  # Updated\nDarkPose_cfg.PIN_MEMORY = True\nDarkPose_cfg.RANK = 0\n\n# Cudnn related params\nDarkPose_cfg.CUDNN = CN()\nDarkPose_cfg.CUDNN.BENCHMARK = True  # Updated\nDarkPose_cfg.CUDNN.DETERMINISTIC = False  # Updated\nDarkPose_cfg.CUDNN.ENABLED = True  # Updated\n\n# common params for NETWORK\nDarkPose_cfg.MODEL = CN()\nDarkPose_cfg.MODEL.NAME = 'pose_hrnet'\nDarkPose_cfg.MODEL.INIT_WEIGHTS = True\nDarkPose_cfg.MODEL.PRETRAINED = '/kaggle/input/cva-models/dark_w32_256×256.pth'  # Updated\nDarkPose_cfg.MODEL.NUM_JOINTS = 16\nDarkPose_cfg.MODEL.TAG_PER_JOINT = True\nDarkPose_cfg.MODEL.TARGET_TYPE = 'gaussian'\nDarkPose_cfg.MODEL.IMAGE_SIZE = [256, 256]\nDarkPose_cfg.MODEL.HEATMAP_SIZE = [64, 64]\nDarkPose_cfg.MODEL.SIGMA = 2\n\nDarkPose_cfg.MODEL.EXTRA = CN()\nDarkPose_cfg.MODEL.EXTRA.NUM_FEATURES = 256\nDarkPose_cfg.MODEL.EXTRA.NUM_STACKS = 8\nDarkPose_cfg.MODEL.EXTRA.NUM_BLOCKS = 1\nDarkPose_cfg.MODEL.EXTRA.NUM_CLASSES = 16\nDarkPose_cfg.MODEL.EXTRA.PRETRAINED_LAYERS = [\n  'conv1',\n  'bn1',\n  'conv2',\n  'bn2',\n  'layer1',\n  'transition1',\n  'stage2',\n  'transition2',\n  'stage3',\n  'transition3',\n  'stage4'\n]\nDarkPose_cfg.MODEL.EXTRA.FINAL_CONV_KERNEL = 1\n\nDarkPose_cfg.MODEL.EXTRA.STAGE2 = CN()\nDarkPose_cfg.MODEL.EXTRA.STAGE2.NUM_MODULES = 1\nDarkPose_cfg.MODEL.EXTRA.STAGE2.NUM_BRANCHES = 2\nDarkPose_cfg.MODEL.EXTRA.STAGE2.NUM_BLOCKS = [4, 4]\nDarkPose_cfg.MODEL.EXTRA.STAGE2.NUM_CHANNELS = [32, 64]\nDarkPose_cfg.MODEL.EXTRA.STAGE2.BLOCK = 'BASIC'\nDarkPose_cfg.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n\nDarkPose_cfg.MODEL.EXTRA.STAGE3 = CN()\nDarkPose_cfg.MODEL.EXTRA.STAGE3.NUM_MODULES = 4\nDarkPose_cfg.MODEL.EXTRA.STAGE3.NUM_BRANCHES = 3\nDarkPose_cfg.MODEL.EXTRA.STAGE3.NUM_BLOCKS = [4, 4, 4]\nDarkPose_cfg.MODEL.EXTRA.STAGE3.NUM_CHANNELS = [32, 64, 128]\nDarkPose_cfg.MODEL.EXTRA.STAGE3.BLOCK = 'BASIC'\nDarkPose_cfg.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n\nDarkPose_cfg.MODEL.EXTRA.STAGE4 = CN()\nDarkPose_cfg.MODEL.EXTRA.STAGE4.NUM_MODULES = 3\nDarkPose_cfg.MODEL.EXTRA.STAGE4.NUM_BRANCHES = 4\nDarkPose_cfg.MODEL.EXTRA.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\nDarkPose_cfg.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [32, 64, 128, 256]\nDarkPose_cfg.MODEL.EXTRA.STAGE4.BLOCK = 'BASIC'\nDarkPose_cfg.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n\nDarkPose_cfg.LOSS = CN()\nDarkPose_cfg.LOSS.USE_TARGET_WEIGHT = True\nDarkPose_cfg.LOSS.USE_DIFFERENT_JOINTS_WEIGHT = False\n# DATASET related params\nDarkPose_cfg.DATASET = CN()\nDarkPose_cfg.DATASET.ROOT = '/kaggle/input/mpii-2014'  # Updated\nDarkPose_cfg.DATASET.DATASET = 'mpii'\nDarkPose_cfg.DATASET.TRAIN_SET = 'train'\nDarkPose_cfg.DATASET.TEST_SET = 'valid'\nDarkPose_cfg.DATASET.DATA_FORMAT = 'jpg'\nDarkPose_cfg.DATASET.HYBRID_JOINTS_TYPE = ''\nDarkPose_cfg.DATASET.SELECT_DATA = False\n\n# training data augmentation\nDarkPose_cfg.DATASET.COLOR_RGB = True  # Updated\nDarkPose_cfg.DATASET.FLIP = True  # Updated\nDarkPose_cfg.DATASET.NUM_JOINTS_HALF_BODY = 8  # Updated\nDarkPose_cfg.DATASET.PROB_HALF_BODY = -1.0  # Updated\nDarkPose_cfg.DATASET.ROT_FACTOR = 30  # Updated\nDarkPose_cfg.DATASET.SCALE_FACTOR = 0.25  # Updated\n\n# train\nDarkPose_cfg.TRAIN = CN()\n\nDarkPose_cfg.TRAIN.LR_FACTOR = 0.1\nDarkPose_cfg.TRAIN.LR_STEP = [170, 200]\nDarkPose_cfg.TRAIN.LR = 0.001\n\nDarkPose_cfg.TRAIN.OPTIMIZER = 'adam'\nDarkPose_cfg.TRAIN.MOMENTUM = 0.9\nDarkPose_cfg.TRAIN.WD = 0.0001\nDarkPose_cfg.TRAIN.NESTEROV = False\nDarkPose_cfg.TRAIN.GAMMA1 = 0.99\nDarkPose_cfg.TRAIN.GAMMA2 = 0.0\n\nDarkPose_cfg.TRAIN.BEGIN_EPOCH = 0\nDarkPose_cfg.TRAIN.END_EPOCH = 10  # Updated\n\nDarkPose_cfg.TRAIN.RESUME = False\nDarkPose_cfg.TRAIN.CHECKPOINT = ''\n\nDarkPose_cfg.TRAIN.BATCH_SIZE_PER_GPU = 32\nDarkPose_cfg.TRAIN.SHUFFLE = True\n\n# testing\nDarkPose_cfg.TEST = CN()\nDarkPose_cfg.TEST.MODEL_FILE = '/kaggle/input/cva-models/dark_w32_256×256.pth'  # Updated\nDarkPose_cfg.TEST.BATCH_SIZE_PER_GPU = 32\nDarkPose_cfg.TEST.FLIP_TEST = True\nDarkPose_cfg.TEST.POST_PROCESS = True\nDarkPose_cfg.TEST.BLUR_KERNEL = 11  # Updated\nDarkPose_cfg.TEST.SHIFT_HEATMAP = True\nDarkPose_cfg.TEST.USE_GT_BBOX = False\n\n# debug\nDarkPose_cfg.DEBUG = CN()\nDarkPose_cfg.DEBUG.DEBUG = False  # Updated\nDarkPose_cfg.DEBUG.SAVE_BATCH_IMAGES_GT = False  # Updated\nDarkPose_cfg.DEBUG.SAVE_BATCH_IMAGES_PRED = False  # Updated\nDarkPose_cfg.DEBUG.SAVE_HEATMAPS_GT = False  # Updated\nDarkPose_cfg.DEBUG.SAVE_HEATMAPS_PRED = False\n\nBN_MOMENTUM = 0.1","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:12.602954Z","iopub.execute_input":"2024-06-25T09:33:12.603234Z","iopub.status.idle":"2024-06-25T09:33:12.628307Z","shell.execute_reply.started":"2024-06-25T09:33:12.603189Z","shell.execute_reply":"2024-06-25T09:33:12.627436Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Class Definition","metadata":{}},{"cell_type":"markdown","source":"## Resnet","metadata":{}},{"cell_type":"code","source":"BN_MOMENTUM = 0.1\n\ndef res_conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass ResBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(ResBasicBlock, self).__init__()\n        self.conv1 = res_conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = res_conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(ResBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                                  momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResBottleneck_CAFFE(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(ResBottleneck_CAFFE, self).__init__()\n        # add stride to conv1x1\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                                  momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass PoseResNet(nn.Module):\n\n    def __init__(self, block, layers, cfg, **kwargs):\n        self.inplanes = 64\n        extra = cfg.MODEL.EXTRA\n        self.deconv_with_bias = extra.DECONV_WITH_BIAS\n\n        super(PoseResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        # used for deconv layers\n        self.deconv_layers = self._make_deconv_layer(\n            extra.NUM_DECONV_LAYERS,\n            extra.NUM_DECONV_FILTERS,\n            extra.NUM_DECONV_KERNELS,\n        )\n\n        self.final_layer = nn.Conv2d(\n            in_channels=extra.NUM_DECONV_FILTERS[-1],\n            out_channels=cfg.MODEL.NUM_JOINTS,\n            kernel_size=extra.FINAL_CONV_KERNEL,\n            stride=1,\n            padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0\n        )\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _get_deconv_cfg(self, deconv_kernel, index):\n        if deconv_kernel == 4:\n            padding = 1\n            output_padding = 0\n        elif deconv_kernel == 3:\n            padding = 1\n            output_padding = 1\n        elif deconv_kernel == 2:\n            padding = 0\n            output_padding = 0\n\n        return deconv_kernel, padding, output_padding\n\n    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n        assert num_layers == len(num_filters), \\\n            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n        assert num_layers == len(num_kernels), \\\n            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n\n        layers = []\n        for i in range(num_layers):\n            kernel, padding, output_padding = \\\n                self._get_deconv_cfg(num_kernels[i], i)\n\n            planes = num_filters[i]\n            layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=self.inplanes,\n                    out_channels=planes,\n                    kernel_size=kernel,\n                    stride=2,\n                    padding=padding,\n                    output_padding=output_padding,\n                    bias=self.deconv_with_bias))\n            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n            layers.append(nn.ReLU(inplace=True))\n            self.inplanes = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.deconv_layers(x)\n        x = self.final_layer(x)\n\n        return x\n\n    def init_weights(self, pretrained=''):\n        if os.path.isfile(pretrained):\n            print('Load pretrained model successfully!')\n            for name, m in self.deconv_layers.named_modules():\n                if isinstance(m, nn.ConvTranspose2d):\n                    nn.init.normal_(m.weight, std=0.001)\n                    if self.deconv_with_bias:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n            for m in self.final_layer.modules():\n                if isinstance(m, nn.Conv2d):\n                    # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                    nn.init.normal_(m.weight, std=0.001)\n                    nn.init.constant_(m.bias, 0)\n\n            # pretrained_state_dict = torch.load(pretrained)\n            # self.load_state_dict(pretrained_state_dict, strict=False)\n            checkpoint = torch.load(pretrained)\n            if isinstance(checkpoint, OrderedDict):\n                state_dict = checkpoint\n            elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n                state_dict_old = checkpoint['state_dict']\n                state_dict = OrderedDict()\n                # delete 'module.' because it is saved from DataParallel module\n                for key in state_dict_old.keys():\n                    if key.startswith('module.'):\n                        # state_dict[key[7:]] = state_dict[key]\n                        # state_dict.pop(key)\n                        state_dict[key[7:]] = state_dict_old[key]\n                    else:\n                        state_dict[key] = state_dict_old[key]\n            else:\n                raise RuntimeError(\n                    'No state_dict found in checkpoint file {}'.format(pretrained))\n            self.load_state_dict(state_dict, strict=False)\n        else:\n            raise ValueError('imagenet pretrained model does not exist')\n\n\nresnet_spec = {50: (ResBottleneck, [3, 4, 6, 3])}\n\n\ndef get_res_pose_net(cfg, is_train, **kwargs):\n    num_layers = cfg.MODEL.EXTRA.NUM_LAYERS\n    style = cfg.MODEL.STYLE\n\n    block_class, layers = resnet_spec[num_layers]\n\n    if style == 'caffe':\n        block_class = ResBottleneck_CAFFE\n\n    model = PoseResNet(block_class, layers, cfg, **kwargs)\n\n    if is_train and cfg.MODEL.INIT_WEIGHTS:\n        model.init_weights(cfg.MODEL.PRETRAINED)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:12.629591Z","iopub.execute_input":"2024-06-25T09:33:12.629909Z","iopub.status.idle":"2024-06-25T09:33:12.676217Z","shell.execute_reply.started":"2024-06-25T09:33:12.629857Z","shell.execute_reply":"2024-06-25T09:33:12.675470Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## HRnet","metadata":{}},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                                  momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(True)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(\n                    num_channels[branch_index] * block.expansion,\n                    momentum=BN_MOMENTUM\n                ),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.num_inchannels[branch_index],\n                num_channels[branch_index],\n                stride,\n                downsample\n            )\n        )\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index]\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels)\n            )\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_inchannels[j],\n                                num_inchannels[i],\n                                1, 1, 0, bias=False\n                            ),\n                            nn.BatchNorm2d(num_inchannels[i]),\n                            nn.Upsample(scale_factor=2**(j-i), mode='nearest')\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i-j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3)\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                    nn.ReLU(True)\n                                )\n                            )\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass PoseHighResolutionNet(nn.Module):\n\n    def __init__(self, cfg, **kwargs):\n        self.inplanes = 64\n        extra = cfg['MODEL']['EXTRA']\n        super(PoseHighResolutionNet, self).__init__()\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(Bottleneck, 64, 4)\n\n        self.stage2_cfg = extra['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition1 = self._make_transition_layer([256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = extra['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = extra['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=False)\n\n        self.final_layer = nn.Conv2d(\n            in_channels=pre_stage_channels[0],\n            out_channels=cfg['MODEL']['NUM_JOINTS'],\n            kernel_size=extra['FINAL_CONV_KERNEL'],\n            stride=1,\n            padding=1 if extra['FINAL_CONV_KERNEL'] == 3 else 0\n        )\n\n        self.pretrained_layers = extra['PRETRAINED_LAYERS']\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                3, 1, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i+1-num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i-num_branches_pre else inchannels\n                    conv3x3s.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                inchannels, outchannels, 3, 2, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(outchannels),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes, planes * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HighResolutionModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    fuse_method,\n                    reset_multi_scale_output\n                )\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        x = self.final_layer(y_list[0])\n\n        return x\n\n    def init_weights(self, pretrained=''):\n        logger.info('=> init weights from normal distribution')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in ['bias']:\n                        nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.ConvTranspose2d):\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in ['bias']:\n                        nn.init.constant_(m.bias, 0)\n\n        if os.path.isfile(pretrained):\n            pretrained_state_dict = torch.load(pretrained)\n            logger.info('=> loading pretrained model {}'.format(pretrained))\n\n            need_init_state_dict = {}\n            for name, m in pretrained_state_dict.items():\n                if name.split('.')[0] in self.pretrained_layers \\\n                   or self.pretrained_layers[0] is '*':\n                    need_init_state_dict[name] = m\n            self.load_state_dict(need_init_state_dict, strict=False)\n        elif pretrained:\n            logger.error('=> please download pre-trained models first!')\n            raise ValueError('{} is not exist!'.format(pretrained))\n\n\ndef get_pose_net(cfg, is_train, **kwargs):\n    model = PoseHighResolutionNet(cfg, **kwargs)\n\n    if is_train and cfg['MODEL']['INIT_WEIGHTS']:\n        model.init_weights(cfg['MODEL']['PRETRAINED'])\n\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-25T09:33:12.677496Z","iopub.execute_input":"2024-06-25T09:33:12.677766Z","iopub.status.idle":"2024-06-25T09:33:12.749623Z","shell.execute_reply.started":"2024-06-25T09:33:12.677734Z","shell.execute_reply":"2024-06-25T09:33:12.748699Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"<>:468: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:468: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n/tmp/ipykernel_34/3729986417.py:468: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  or self.pretrained_layers[0] is '*':\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## DarkPose","metadata":{}},{"cell_type":"code","source":"darklogger = logging.getLogger(__name__)\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass DarkBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(DarkBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DarkBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(DarkBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                                  momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DarkHighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(DarkHighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(True)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            darklogger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            darklogger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            darklogger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(\n                    num_channels[branch_index] * block.expansion,\n                    momentum=BN_MOMENTUM\n                ),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.num_inchannels[branch_index],\n                num_channels[branch_index],\n                stride,\n                downsample\n            )\n        )\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index]\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels)\n            )\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_inchannels[j],\n                                num_inchannels[i],\n                                1, 1, 0, bias=False\n                            ),\n                            nn.BatchNorm2d(num_inchannels[i]),\n                            nn.Upsample(scale_factor=2**(j-i), mode='nearest')\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i-j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3)\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                    nn.ReLU(True)\n                                )\n                            )\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': DarkBasicBlock,\n    'BOTTLENECK': DarkBottleneck\n}\n\n\nclass DarkPoseHighResolutionNet(nn.Module):\n\n    def __init__(self, cfg, **kwargs):\n        self.inplanes = 64\n        extra = cfg.MODEL.EXTRA\n        super(DarkPoseHighResolutionNet, self).__init__()\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(DarkBottleneck, 64, 4)\n\n        self.stage2_cfg = cfg['MODEL']['EXTRA']['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition1 = self._make_transition_layer([256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg['MODEL']['EXTRA']['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg['MODEL']['EXTRA']['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=False)\n\n        self.final_layer = nn.Conv2d(\n            in_channels=pre_stage_channels[0],\n            out_channels=cfg.MODEL.NUM_JOINTS,\n            kernel_size=extra.FINAL_CONV_KERNEL,\n            stride=1,\n            padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0\n        )\n\n        self.pretrained_layers = cfg['MODEL']['EXTRA']['PRETRAINED_LAYERS']\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                3, 1, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i+1-num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i-num_branches_pre else inchannels\n                    conv3x3s.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                inchannels, outchannels, 3, 2, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(outchannels),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes, planes * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                DarkHighResolutionModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    fuse_method,\n                    reset_multi_scale_output\n                )\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        x = self.final_layer(y_list[0])\n\n        return x\n\n    def init_weights(self, pretrained=''):\n        darklogger.info('=> init weights from normal distribution')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in ['bias']:\n                        nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.ConvTranspose2d):\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in ['bias']:\n                        nn.init.constant_(m.bias, 0)\n\n        if os.path.isfile(pretrained):\n            pretrained_state_dict = torch.load(pretrained)\n            darklogger.info('=> loading pretrained model {}'.format(pretrained))\n\n            need_init_state_dict = {}\n            for name, m in pretrained_state_dict.items():\n                if name.split('.')[0] in self.pretrained_layers \\\n                   or self.pretrained_layers[0] is '*':\n                    need_init_state_dict[name] = m\n            self.load_state_dict(need_init_state_dict, strict=False)\n        elif pretrained:\n            darklogger.error('=> please download pre-trained models first!')\n            raise ValueError('{} is not exist!'.format(pretrained))\n\n\ndef get_dark_pose_net(cfg, is_train, **kwargs):\n    model = DarkPoseHighResolutionNet(cfg, **kwargs)\n\n    if is_train and cfg.MODEL.INIT_WEIGHTS:\n        model.init_weights(cfg.MODEL.PRETRAINED)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:12.750953Z","iopub.execute_input":"2024-06-25T09:33:12.751241Z","iopub.status.idle":"2024-06-25T09:33:12.821958Z","shell.execute_reply.started":"2024-06-25T09:33:12.751217Z","shell.execute_reply":"2024-06-25T09:33:12.821130Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"<>:468: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:468: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n/tmp/ipykernel_34/3864332082.py:468: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  or self.pretrained_layers[0] is '*':\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Function definition","metadata":{}},{"cell_type":"code","source":"def dark_transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = dark_get_affine_transform(center, scale, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = dark_affine_transform(coords[p, 0:2], trans)\n    return target_coords\n\n\ndef dark_get_affine_transform(\n        center, scale, rot, output_size,\n        shift=np.array([0, 0], dtype=np.float32), inv=0\n):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        print(scale)\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 200.0\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n\n    src_dir = dark_get_dir([0, (src_w-1) * -0.5], rot_rad)\n    dst_dir = np.array([0, (dst_w-1) * -0.5], np.float32)\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [(dst_w-1) * 0.5, (dst_h-1) * 0.5]\n    dst[1, :] = np.array([(dst_w-1) * 0.5, (dst_h-1) * 0.5]) + dst_dir\n\n    src[2:, :] = dark_get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = dark_get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef dark_affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef dark_get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef dark_get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\ndef dark_get_max_preds(batch_heatmaps):\n    '''\n    get predictions from score maps\n    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n    '''\n    assert isinstance(batch_heatmaps, np.ndarray), \\\n        'batch_heatmaps should be numpy.ndarray'\n    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width = batch_heatmaps.shape[3]\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n    idx = np.argmax(heatmaps_reshaped, 2)\n    maxvals = np.amax(heatmaps_reshaped, 2)\n\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n    idx = idx.reshape((batch_size, num_joints, 1))\n\n    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n\n    preds[:, :, 0] = (preds[:, :, 0]) % width\n    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n    pred_mask = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef dark_taylor(hm, coord):\n    heatmap_height = hm.shape[0]\n    heatmap_width = hm.shape[1]\n    px = int(coord[0])\n    py = int(coord[1])\n    if 1 < px < heatmap_width-2 and 1 < py < heatmap_height-2:\n        dx  = 0.5 * (hm[py][px+1] - hm[py][px-1])\n        dy  = 0.5 * (hm[py+1][px] - hm[py-1][px])\n        dxx = 0.25 * (hm[py][px+2] - 2 * hm[py][px] + hm[py][px-2])\n        dxy = 0.25 * (hm[py+1][px+1] - hm[py-1][px+1] - hm[py+1][px-1] \\\n            + hm[py-1][px-1])\n        dyy = 0.25 * (hm[py+2*1][px] - 2 * hm[py][px] + hm[py-2*1][px])\n        derivative = np.matrix([[dx],[dy]])\n        hessian = np.matrix([[dxx,dxy],[dxy,dyy]])\n        if dxx * dyy - dxy ** 2 != 0:\n            hessianinv = hessian.I\n            offset = -hessianinv * derivative\n            offset = np.squeeze(np.array(offset.T), axis=0)\n            coord += offset\n    return coord\n\n\ndef dark_gaussian_blur(hm, kernel):\n    border = (kernel - 1) // 2\n    batch_size = hm.shape[0]\n    num_joints = hm.shape[1]\n    height = hm.shape[2]\n    width = hm.shape[3]\n    for i in range(batch_size):\n        for j in range(num_joints):\n            origin_max = np.max(hm[i,j])\n            dr = np.zeros((height + 2 * border, width + 2 * border))\n            dr[border: -border, border: -border] = hm[i,j].copy()\n            dr = cv2.GaussianBlur(dr, (kernel, kernel), 0)\n            hm[i,j] = dr[border: -border, border: -border].copy()\n            hm[i,j] *= origin_max / np.max(hm[i,j])\n    return hm\n\n\ndef dark_get_final_preds(config, hm, center, scale):\n    coords, maxvals = dark_get_max_preds(hm)\n    heatmap_height = hm.shape[2]\n    heatmap_width = hm.shape[3]\n\n    # post-processing\n    hm = dark_gaussian_blur(hm, config.TEST.BLUR_KERNEL)\n    hm = np.maximum(hm, 1e-10)\n    hm = np.log(hm)\n    for n in range(coords.shape[0]):\n        for p in range(coords.shape[1]):\n            coords[n,p] = dark_taylor(hm[n][p], coords[n][p])\n\n    preds = coords.copy()\n\n    # Transform back\n    for i in range(coords.shape[0]):\n        preds[i] = dark_transform_preds(\n            coords[i], center[i], scale[i], [heatmap_width, heatmap_height]\n        )\n\n    return preds, maxvals","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:12.825134Z","iopub.execute_input":"2024-06-25T09:33:12.825386Z","iopub.status.idle":"2024-06-25T09:33:12.995094Z","shell.execute_reply.started":"2024-06-25T09:33:12.825362Z","shell.execute_reply":"2024-06-25T09:33:12.994062Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def res_get_max_preds(batch_heatmaps):\n    '''\n    get predictions from score maps\n    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n    '''\n    assert isinstance(batch_heatmaps, np.ndarray), \\\n        'batch_heatmaps should be numpy.ndarray'\n    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width = batch_heatmaps.shape[3]\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n    idx = np.argmax(heatmaps_reshaped, 2)\n    maxvals = np.amax(heatmaps_reshaped, 2)\n\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n    idx = idx.reshape((batch_size, num_joints, 1))\n\n    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n\n    preds[:, :, 0] = (preds[:, :, 0]) % width\n    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n    pred_mask = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef res_get_final_preds(config, batch_heatmaps, center, scale):\n    coords, maxvals = res_get_max_preds(batch_heatmaps)\n\n    heatmap_height = batch_heatmaps.shape[2]\n    heatmap_width = batch_heatmaps.shape[3]\n\n    preds = coords.copy()\n\n    # Transform back\n    for i in range(coords.shape[0]):\n        preds[i] = res_transform_preds(coords[i], center, scale,\n                                   [heatmap_width, heatmap_height])\n\n    return preds, maxvals\n\ndef res_transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = res_get_affine_transform(center, scale, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = res_affine_transform(coords[p, 0:2], trans)\n    return target_coords\n\n\ndef res_get_affine_transform(center,\n                         scale,\n                         rot,\n                         output_size,\n                         shift=np.array([0, 0], dtype=np.float32),\n                         inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        print(scale)\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 200.0\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    src_dir = res_get_dir([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = res_get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = res_get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef res_affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef res_get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef res_get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:12.996260Z","iopub.execute_input":"2024-06-25T09:33:12.996566Z","iopub.status.idle":"2024-06-25T09:33:13.021827Z","shell.execute_reply.started":"2024-06-25T09:33:12.996540Z","shell.execute_reply":"2024-06-25T09:33:13.020904Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\ndef get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\ndef get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\ndef get_affine_transform(\n        center, scale, rot, output_size,\n        shift=np.array([0, 0], dtype=np.float32), inv=0\n):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        print(scale)\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 200.0\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\ndef transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n    return target_coords\n\ndef get_max_preds(batch_heatmaps):\n    '''\n    get predictions from score maps\n    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n    '''\n    assert isinstance(batch_heatmaps, np.ndarray), \\\n        'batch_heatmaps should be numpy.ndarray'\n    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width = batch_heatmaps.shape[3]\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n    idx = np.argmax(heatmaps_reshaped, 2)\n    maxvals = np.amax(heatmaps_reshaped, 2)\n\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n    idx = idx.reshape((batch_size, num_joints, 1))\n\n    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n\n    preds[:, :, 0] = (preds[:, :, 0]) % width\n    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n    pred_mask = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef get_final_preds(config, batch_heatmaps, center, scale):\n    coords, maxvals = get_max_preds(batch_heatmaps)\n\n    heatmap_height = batch_heatmaps.shape[2]\n    heatmap_width = batch_heatmaps.shape[3]\n\n    # post-processing\n    if config.TEST.POST_PROCESS:\n        for n in range(coords.shape[0]):\n            for p in range(coords.shape[1]):\n                hm = batch_heatmaps[n][p]\n                px = int(math.floor(coords[n][p][0] + 0.5))\n                py = int(math.floor(coords[n][p][1] + 0.5))\n                if 1 < px < heatmap_width-1 and 1 < py < heatmap_height-1:\n                    diff = np.array(\n                        [\n                            hm[py][px+1] - hm[py][px-1],\n                            hm[py+1][px]-hm[py-1][px]\n                        ]\n                    )\n                    coords[n][p] += np.sign(diff) * .25\n\n    preds = coords.copy()\n\n    # Transform back\n    for i in range(coords.shape[0]):\n        preds[i] = transform_preds(\n            coords[i], center[i], scale[i], [heatmap_width, heatmap_height]\n        )\n\n    return preds, maxvals\n\ndef preprocess_image(img_path, c, s, cfg):\n    flip_pairs = [[0, 5], [1, 4], [\n            2, 3], [10, 15], [11, 14], [12, 13]]\n\n    scale_factor = cfg.DATASET.SCALE_FACTOR\n    rotation_factor = cfg.DATASET.ROT_FACTOR\n    flip = cfg.DATASET.FLIP\n\n    image_size = cfg.MODEL.IMAGE_SIZE\n    target_type = cfg.MODEL.EXTRA.TARGET_TYPE\n    heatmap_size = cfg.MODEL.EXTRA.HEATMAP_SIZE\n    sigma = cfg.MODEL.EXTRA.SIGMA\n\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),          \n        transforms.Normalize(            \n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n\n    data_numpy = img_path\n    r = 0\n\n    trans = res_get_affine_transform(c, s, r, image_size)\n    input = cv2.warpAffine(\n        data_numpy,\n        trans,\n        (int(image_size[0]), int(image_size[1])),\n        flags=cv2.INTER_LINEAR)\n\n    input = transform(input)\n\n    return input\n\n\ndef predict_keypoints(origin_img, c, s, config, model, num_samples = 1):\n    model.eval()\n\n    all_preds = np.zeros((num_samples, config.MODEL.NUM_JOINTS, 3),\n                         dtype=np.float32)\n                \n    input = preprocess_image(origin_img, c, s, config)\n    input = input.unsqueeze(0)\n    \n    with torch.no_grad():\n        output = model(input)\n\n        num_images = input.size(0)\n  \n        preds, maxvals = res_get_final_preds(\n            config, output.clone().cpu().numpy(), c, s)\n        \n        idx = 0\n        all_preds[idx:idx + num_images, :, 0:2] = preds[:, :, 0:2]       \n        all_preds = all_preds[:, :, 0:2] + 1.0\n        \n        return all_preds[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:33:13.023479Z","iopub.execute_input":"2024-06-25T09:33:13.023807Z","iopub.status.idle":"2024-06-25T09:33:13.057862Z","shell.execute_reply.started":"2024-06-25T09:33:13.023773Z","shell.execute_reply":"2024-06-25T09:33:13.057069Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# function definition\ndef cosine_similarity(a, b):\n  cos = ( np.dot(a, b) / \n          (np.linalg.norm(a) * np.linalg.norm(b)) )\n  if cos < -1:\n    return -1\n  elif cos > 1:\n    return 1\n  else:\n    return cos\n\n\ndef angle_degree(a, b):\n  angle = (np.arccos(cosine_similarity(a, b)) / \n          np.pi) * 180\n  if angle > 180:\n    return 180.\n  if angle < 0:\n    return 0\n  else:\n    return angle\n\n# @parameter\n# part: contain 2 joint coordinate [[x0, y0], [x1, y1]]\ndef return_vector(part_id, skeleton):\n  point_a = skeleton[part_id[0]]\n  point_b = skeleton[part_id[1]]\n  return point_a - point_b\n\ndef is_in_range(num, range1, range2):\n  ceil = max(range1, range2)\n  floor = min(range1, range2)\n  return (num >= floor and num <= ceil)\n\ndef find_pose_id(ske_pred, keyparts, stages):\n  assert ske_pred.shape == (NUM_KPTS, 2)\n  avg_angle = 0\n  for keypart in keyparts:\n    part_a, part_b = keypart\n    vec_a = return_vector(SKELETON[part_a], ske_pred)\n    vec_b = return_vector(SKELETON[part_b], ske_pred)\n\n    if sum(vec_a == 0) == 2 or sum(vec_b == 0) == 2:\n      return -1\n    \n    pred_angle = angle_degree(vec_a, vec_b)\n    avg_angle += pred_angle\n\n  avg_angle /= len(keyparts)\n  for i, stage in enumerate(stages):\n    if is_in_range(avg_angle, stage[0], stage[1]):\n      return i\n\ndef compare_skeleton(ske_true, ske_pred, excer_part, threshold= 10):\n  assert ske_true.shape == (NUM_KPTS, 2)\n  assert ske_pred.shape == (NUM_KPTS, 2)\n  part_flag = []\n  for part in excer_part:\n    kpt_a, kpt_b = SKELETON[part]\n    true_vec = ske_true[kpt_a] - ske_true[kpt_b]\n    pred_vec = ske_pred[kpt_a] - ske_pred[kpt_b]\n    angle = angle_degree(true_vec, pred_vec)\n    part_flag.append(True if angle <= threshold else False)\n  return part_flag\n\ndef draw_pose(keypoints, part_flag, excer_part, img, joint_thickness=6):\n  assert keypoints.shape == (NUM_KPTS, 2)\n  for i, part in enumerate(excer_part):\n    kpt_a, kpt_b = SKELETON[part]\n    c = COLOR[\"blue\"] if part_flag[i] else COLOR[\"red\"]\n    x_a, y_a = keypoints[kpt_a]\n    x_b, y_b = keypoints[kpt_b]\n    cv2.circle(img, (int(x_a), int(y_a)), joint_thickness, COLOR[\"green\"], -1)\n    cv2.circle(img, (int(x_b), int(y_b)), joint_thickness, COLOR[\"green\"], -1)\n    cv2.line(img, (int(x_a), int(y_a)), (int(x_b), int(y_b)), c, 2)\n    \ndef dark_get_pose_estimation_prediction(pose_model, cfg, image, center, scale):\n  scale = np.array([scale, scale])\n  rotation = 0\n\n  # pose estimation transformation\n  trans = dark_get_affine_transform(center, scale, rotation, cfg.MODEL.IMAGE_SIZE)\n  model_input = cv2.warpAffine(\n    image,\n    trans,\n    (int(cfg.MODEL.IMAGE_SIZE[0]), int(cfg.MODEL.IMAGE_SIZE[1])),\n    flags=cv2.INTER_LINEAR)\n  transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                          std=[0.229, 0.224, 0.225]),\n  ])\n\n  # pose estimation inference\n  model_input = transform(model_input).unsqueeze(0)\n  # switch to evaluate mode\n  pose_model.eval()\n  with torch.no_grad():\n    # compute output heatmap\n    output = pose_model(model_input)\n    preds, _ = dark_get_final_preds(\n      cfg,\n      output.clone().cpu().numpy(),\n      np.asarray([center]),\n      np.asarray([scale]))\n\n    return preds\n\ndef hr_get_pose_estimation_prediction(pose_model, cfg, image, center, scale):\n  scale = np.array([scale, scale])\n  rotation = 0\n\n  # pose estimation transformation\n  trans = get_affine_transform(center, scale, rotation, cfg.MODEL.IMAGE_SIZE)\n  model_input = cv2.warpAffine(\n    image,\n    trans,\n    (int(cfg.MODEL.IMAGE_SIZE[0]), int(cfg.MODEL.IMAGE_SIZE[1])),\n    flags=cv2.INTER_LINEAR)\n  transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                          std=[0.229, 0.224, 0.225]),\n  ])\n\n  # pose estimation inference\n  model_input = transform(model_input).unsqueeze(0)\n  # switch to evaluate mode\n  pose_model.eval()\n  with torch.no_grad():\n    # compute output heatmap\n    output = pose_model(model_input)\n    preds, _ = get_final_preds(\n      cfg,\n      output.clone().cpu().numpy(),\n      np.asarray([center]),\n      np.asarray([scale]))\n\n    return preds\n\ndef calculate_center_scale(box):\n  x1, y1, x2, y2 = box\n  center_x = (x2 + x1) / 2\n  center_y = (y2 + y1) / 2\n  width = x2 - x1\n  height = y2 - y1\n  \n  center = np.array([center_x, center_y], dtype=np.float32)\n  scale = max(width, height) / 200\n\n  return center, scale\n\ndef get_person_box(model, img):\n  results = model(img)\n  \n  data = results.xyxy[0].cpu().numpy()\n    \n  if len(data) == 0:\n    return [-1], [-1]\n\n  max_index = np.argmax(data[:, 4])\n  max_element = data[max_index]\n  bbox = max_element[:4]\n\n  center, scale = calculate_center_scale(bbox)\n\n  return center, scale\n    \ndef load_yolo(version):\n  yolo = torch.hub.load('ultralytics/yolov5', version, \n                        pretrained= True, _verbose= False)\n  yolo.cuda()\n  yolo.classes = [0]\n  return yolo\n\ndef load_res(cfg):\n  model = get_res_pose_net(cfg, is_train=True)\n  gpus = [int(i) for i in poseres_cfg.GPUS.split(',')]\n  model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n  return model\n\ndef load_hrnet(cfg):\n  pose_model = get_pose_net(cfg, is_train=False)\n    \n  print(cfg.TEST.MODEL_FILE)\n  if cfg.TEST.MODEL_FILE:\n    print('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n    pose_model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=False)\n  else:\n    print('expected model defined in config at TEST.MODEL_FILE')\n\n  pose_model = torch.nn.DataParallel(pose_model, device_ids=cfg.GPUS)\n  pose_model.to(CTX)\n  pose_model.eval()\n\n  return pose_model\n\ndef load_dark(cfg):\n  pose_model = get_dark_pose_net(cfg, is_train=False)\n    \n  print(cfg.TEST.MODEL_FILE)\n  if cfg.TEST.MODEL_FILE:\n    print('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n    pose_model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=False)\n  else:\n    print('expected model defined in config at TEST.MODEL_FILE')\n\n  pose_model = torch.nn.DataParallel(pose_model, device_ids=cfg.GPUS)\n  pose_model.to(CTX)\n  pose_model.eval()\n\n  return pose_model\n\ndef load_true_poses(path):\n  joints_path = os.path.join(path, \"joints.csv\")\n  \n  img_path = os.path.join(path, \"images\")\n  \n  img_paths = os.listdir(img_path)\n  img_paths.sort(key= lambda x: int(x[0]))\n  imgs = [cv2.imread(os.path.join(img_path, p)) for p in img_paths]\n  skes = pd.read_csv(joints_path).to_numpy().reshape(len(imgs), NUM_KPTS, 2)\n  return imgs, skes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-25T09:33:13.059232Z","iopub.execute_input":"2024-06-25T09:33:13.059501Z","iopub.status.idle":"2024-06-25T09:33:13.096487Z","shell.execute_reply.started":"2024-06-25T09:33:13.059478Z","shell.execute_reply":"2024-06-25T09:33:13.095674Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def res_video_test(video_path, output_path, box_model, pose_model, true_pose_images, true_pose_skes,\n                   cfg, keyparts, visual_parts, stage_angle, angle_threshold=5):\n\n    # Open video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(\"Cannot open this video.\")\n        return\n\n    # Get video properties\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    # Create video writer\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, \n                          (frame_width * 2, frame_height))\n    count = 0\n    pose_id = 0\n\n    # Read and process frames\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        \n        center, scale = get_person_box(box_model, frame_rgb)\n        if center[0] != -1:\n            pose_preds = predict_keypoints(frame_rgb, np.array(center), \n                                           np.array([scale, scale]), cfg, pose_model)\n\n            pose_id = find_pose_id(pose_preds, keyparts, stage_angle)\n\n            true_pose_image = true_pose_images[pose_id]\n            true_skeleton = true_pose_skes[pose_id]\n            part_flag = compare_skeleton(true_skeleton, pose_preds, visual_parts, threshold=5)\n            draw_pose(pose_preds, part_flag, visual_parts, frame)\n\n            # Resize true pose image to match the frame size\n            true_pose_resized = cv2.resize(true_pose_image, (frame_width, frame_height))\n\n            # Combine frames side by side\n            combined_frame = np.hstack((frame, true_pose_resized))\n        else:\n            true_pose_image = true_pose_images[0]\n            true_pose_resized = cv2.resize(true_pose_image, (frame_width, frame_height))\n            combined_frame = np.hstack((frame, true_pose_resized))\n\n        # Write combined frame to output video\n        out.write(combined_frame)\n\n\n    cap.release()\n    out.release()\n    print(\"Video processing complete. Output saved to\", output_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:48:15.632612Z","iopub.execute_input":"2024-06-25T09:48:15.633352Z","iopub.status.idle":"2024-06-25T09:48:15.645407Z","shell.execute_reply.started":"2024-06-25T09:48:15.633319Z","shell.execute_reply":"2024-06-25T09:48:15.644462Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def hr_video_test(video_path, output_path, box_model, pose_model, true_pose_images, true_pose_skes,\n                   cfg, keyparts, visual_parts, stage_angle, angle_threshold=5):\n  \n  video_cap = cv2.VideoCapture(video_path)\n  if not video_cap.isOpened():\n    print(\"Cannot open this video.\")\n    return\n  \n  frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n  frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n  fps = 24.0\n  video_out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), \n                        fps, (frame_width * 2, frame_height))\n  canvas_size = (frame_height, frame_width, 3)\n  canvas = np.zeros(canvas_size)\n  count = 0\n  while True:\n    pose_id = 0\n      \n    ret, image_bgr = video_cap.read()\n    if not ret:\n      break\n    \n    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    center, scale = get_person_box(box_model, image)\n\n    if center[0] == -1:\n     continue\n\n    image_pose = image.copy() if cfg.DATASET.COLOR_RGB else image_bgr.copy()\n    pose_pred = hr_get_pose_estimation_prediction(pose_model, cfg, image_pose, center, scale)[0]\n    pose_id = find_pose_id(pose_pred, keyparts, stage_angle)\n    if pose_id == -1:\n      continue\n    \n    true_pose_image = true_pose_images[pose_id]\n    true_skeleton = true_pose_skes[pose_id]\n    part_flag = compare_skeleton(true_skeleton, pose_pred, visual_parts, threshold= angle_threshold)\n    draw_pose(pose_pred, part_flag, visual_parts, image_bgr)\n    \n    true_pose_resized = cv2.resize(true_pose_image, (frame_width, frame_height))\n    combined_frame = np.hstack((image_bgr, true_pose_resized)).astype(np.uint8)\n    video_out.write(combined_frame)\n    \n    count += 1\n    if count % 24 == 0:\n      print(count)\n  \n  video_cap.release()\n  video_out.release()\n  print(\"Video processing complete. Output saved to\", output_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:55:42.643464Z","iopub.execute_input":"2024-06-25T09:55:42.644204Z","iopub.status.idle":"2024-06-25T09:55:42.655962Z","shell.execute_reply.started":"2024-06-25T09:55:42.644171Z","shell.execute_reply":"2024-06-25T09:55:42.654826Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def dark_video_test(video_path, output_path, box_model, pose_model, true_pose_images, true_pose_skes,\n                   cfg, keyparts, visual_parts, stage_angle, angle_threshold=5):\n  \n  video_cap = cv2.VideoCapture(video_path)\n  if not video_cap.isOpened():\n    print(\"Cannot open this video.\")\n    return\n  \n  frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n  frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n  fps = 24.0\n  video_out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), \n                        fps, (frame_width * 2, frame_height))\n  canvas_size = (frame_height, frame_width, 3)\n  canvas = np.zeros(canvas_size)\n  count = 0\n  while True:\n    pose_id = 0\n      \n    ret, image_bgr = video_cap.read()\n    if not ret:\n      break\n    \n    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    center, scale = get_person_box(box_model, image)\n\n    if center[0] == -1:\n     continue\n\n    image_pose = image.copy() if cfg.DATASET.COLOR_RGB else image_bgr.copy()\n    pose_pred = dark_get_pose_estimation_prediction(pose_model, cfg, image_pose, center, scale)[0]\n    pose_id = find_pose_id(pose_pred, keyparts, stage_angle)\n    if pose_id == -1:\n      continue\n    \n    true_pose_image = true_pose_images[pose_id]\n    true_skeleton = true_pose_skes[pose_id]\n    part_flag = compare_skeleton(true_skeleton, pose_pred, visual_parts, threshold= angle_threshold)\n    draw_pose(pose_pred, part_flag, visual_parts, image_bgr)\n\n    true_pose_resized = cv2.resize(true_pose_image, (frame_width, frame_height))\n    combined_frame = np.hstack((image_bgr, true_pose_resized)).astype(np.uint8)\n    video_out.write(combined_frame)\n    \n    count += 1\n    if count % 24 == 0:\n      print(count)\n  \n  video_cap.release()\n  video_out.release()\n  print(\"Video processing complete. Output saved to\", output_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:08:09.588621Z","iopub.execute_input":"2024-06-25T10:08:09.589045Z","iopub.status.idle":"2024-06-25T10:08:09.601742Z","shell.execute_reply.started":"2024-06-25T10:08:09.589014Z","shell.execute_reply":"2024-06-25T10:08:09.600642Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Demo","metadata":{}},{"cell_type":"code","source":"squat_dir = \"/kaggle/input/cva-inputs/squat\"\njumping_jack_dir = \"/kaggle/input/cva-inputs/jumping_jack\"\nbox_model = load_yolo(\"yolov5s\")\nsquat_images, squat_skes = load_true_poses(squat_dir)\njumping_jack_images, jumping_jack_skes = load_true_poses(jumping_jack_dir)\nrespose_model = load_res(poseres_cfg)\nhrpose_model = load_hrnet(HRnet_cfg)\ndarkpose_model = load_dark(DarkPose_cfg)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:42:37.648810Z","iopub.execute_input":"2024-06-25T09:42:37.649750Z","iopub.status.idle":"2024-06-25T09:42:40.782963Z","shell.execute_reply.started":"2024-06-25T09:42:37.649717Z","shell.execute_reply":"2024-06-25T09:42:40.782163Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","output_type":"stream"},{"name":"stdout","text":"Load pretrained model successfully!\n/kaggle/input/cva-models/hrpose_w32_256x256.pth\n=> loading model from /kaggle/input/cva-models/hrpose_w32_256x256.pth\n/kaggle/input/cva-models/dark_w32_256×256.pth\n=> loading model from /kaggle/input/cva-models/dark_w32_256×256.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"squat_input_video = \"/kaggle/input/cva-inputs/stop_doing.mp4\"\njumping_jack_input_video = \"/kaggle/input/cva-inputs/tue_jumping_jack_1.mp4\"","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:44:51.893911Z","iopub.execute_input":"2024-06-25T09:44:51.894686Z","iopub.status.idle":"2024-06-25T09:44:51.898748Z","shell.execute_reply.started":"2024-06-25T09:44:51.894656Z","shell.execute_reply":"2024-06-25T09:44:51.897555Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Pose Resnet","metadata":{}},{"cell_type":"code","source":"res_output_video = \"res_output_video.mp4\"\nres_video_test(squat_input_video, res_output_video, box_model, respose_model, squat_images,\n               squat_skes, poseres_cfg, SQUAT_KEYPART, SQUAT_PART, SQUAT_STAGE_ANGLE)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:51:55.000823Z","iopub.execute_input":"2024-06-25T09:51:55.001623Z","iopub.status.idle":"2024-06-25T09:52:02.396786Z","shell.execute_reply.started":"2024-06-25T09:51:55.001591Z","shell.execute_reply":"2024-06-25T09:52:02.395795Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/2534953332.py:3: RuntimeWarning: invalid value encountered in scalar divide\n  cos = ( np.dot(a, b) /\n/tmp/ipykernel_34/2534953332.py:3: RuntimeWarning: invalid value encountered in scalar divide\n  cos = ( np.dot(a, b) /\n","output_type":"stream"},{"name":"stdout","text":"Video processing complete. Output saved to res_output_video.mp4\n","output_type":"stream"}]},{"cell_type":"code","source":"res_video_test(jumping_jack_input_video, res_output_video, box_model, respose_model, \n               jumping_jack_images, jumping_jack_skes, poseres_cfg, JUMPING_JACK_KEYPART, \n               JUMPING_JACK_PART, JUMPING_JACK_STAGE_ANGLE)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:00:32.343418Z","iopub.execute_input":"2024-06-25T10:00:32.344211Z","iopub.status.idle":"2024-06-25T10:00:42.030742Z","shell.execute_reply.started":"2024-06-25T10:00:32.344177Z","shell.execute_reply":"2024-06-25T10:00:42.029785Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Video processing complete. Output saved to res_output_video.mp4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## HRnet","metadata":{}},{"cell_type":"code","source":"hr_output_video = \"hr_output_video.mp4\"\nhr_video_test(squat_input_video, hr_output_video, box_model, hrpose_model, squat_images,\n               squat_skes, HRnet_cfg, SQUAT_KEYPART, SQUAT_PART, SQUAT_STAGE_ANGLE)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:55:48.576250Z","iopub.execute_input":"2024-06-25T09:55:48.576986Z","iopub.status.idle":"2024-06-25T09:56:06.274187Z","shell.execute_reply.started":"2024-06-25T09:55:48.576955Z","shell.execute_reply":"2024-06-25T09:56:06.273169Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"24\n48\n72\n96\n120\n144\n168\n192\n216\n240\nVideo processing complete. Output saved to hr_output_video.mp4\n","output_type":"stream"}]},{"cell_type":"code","source":"hr_video_test(jumping_jack_input_video, hr_output_video, box_model, hrpose_model, \n              jumping_jack_images, jumping_jack_skes, HRnet_cfg, JUMPING_JACK_KEYPART, \n               JUMPING_JACK_PART, JUMPING_JACK_STAGE_ANGLE)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:05:04.223207Z","iopub.execute_input":"2024-06-25T10:05:04.223916Z","iopub.status.idle":"2024-06-25T10:05:22.698424Z","shell.execute_reply.started":"2024-06-25T10:05:04.223868Z","shell.execute_reply":"2024-06-25T10:05:22.697526Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"24\n48\n72\n96\n120\n144\n168\n192\n216\nVideo processing complete. Output saved to hr_output_video.mp4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## DarkPose","metadata":{}},{"cell_type":"code","source":"dark_output_video = \"dark_output_video.mp4\"\ndark_video_test(squat_input_video, dark_output_video, box_model, hrpose_model, squat_images, \n                squat_skes, DarkPose_cfg, SQUAT_KEYPART, SQUAT_PART, SQUAT_STAGE_ANGLE)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:08:14.589928Z","iopub.execute_input":"2024-06-25T10:08:14.590281Z","iopub.status.idle":"2024-06-25T10:08:34.460322Z","shell.execute_reply.started":"2024-06-25T10:08:14.590256Z","shell.execute_reply":"2024-06-25T10:08:34.459088Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"24\n48\n72\n96\n120\n144\n168\n192\n216\n240\nVideo processing complete. Output saved to dark_output_video.mp4\n","output_type":"stream"}]},{"cell_type":"code","source":"dark_video_test(jumping_jack_input_video, dark_output_video, box_model, hrpose_model, \n                jumping_jack_images, jumping_jack_skes, DarkPose_cfg, JUMPING_JACK_KEYPART, \n                JUMPING_JACK_PART, JUMPING_JACK_STAGE_ANGLE)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T10:16:46.290139Z","iopub.execute_input":"2024-06-25T10:16:46.290574Z","iopub.status.idle":"2024-06-25T10:17:06.249206Z","shell.execute_reply.started":"2024-06-25T10:16:46.290543Z","shell.execute_reply":"2024-06-25T10:17:06.247943Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"24\n48\n72\n96\n120\n144\n168\n192\n216\nVideo processing complete. Output saved to dark_output_video.mp4\n","output_type":"stream"}]}]}